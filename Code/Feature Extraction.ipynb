{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize']=(14,6)\n",
    "# import necessary libraries and specify that graphs should be plotted inline.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.cross_validation import train_test_split,cross_val_score,StratifiedKFold, KFold\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "from sklearn import feature_selection\n",
    "from sklearn import datasets, tree, linear_model, svm, cross_validation\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import precision_score, \\\n",
    "    recall_score, confusion_matrix, classification_report, \\\n",
    "    accuracy_score, f1_score, make_scorer, roc_auc_score, roc_curve\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from mlxtend.feature_selection import plot_sequential_feature_selection as plot_sfs\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from __future__ import print_function\n",
    "import subprocess\n",
    "import time\n",
    "from operator import itemgetter\n",
    "import random\n",
    "random.seed(0)\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN,SMOTETomek\n",
    "from pyxdameraulevenshtein import damerau_levenshtein_distance, normalized_damerau_levenshtein_distance,\\\n",
    "damerau_levenshtein_distance_ndarray, normalized_damerau_levenshtein_distance_ndarray\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import warnings\n",
    "import re\n",
    "import gensim, logging\n",
    "from scipy import spatial\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.cross_validation import train_test_split,cross_val_score,StratifiedKFold, KFold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "desc_clean=pd.read_csv(\"description_cleaned_lemma.csv\")\n",
    "product_train=pd.read_csv(\"product_train_lemma.csv\")\n",
    "desc_clean[\"product_description\"] = desc_clean[\"product_description\"].astype(str)\n",
    "desc_clean[\"product_brand\"] = desc_clean[\"product_brand\"].astype(str)\n",
    "desc_clean[\"product_color\"] = desc_clean[\"product_color\"].astype(str)\n",
    "desc_clean[\"product_attribute_concat\"] = desc_clean[\"product_attribute_concat\"].astype(str)\n",
    "\n",
    "product_train[\"product_title\"] = product_train[\"product_title\"].astype(str)\n",
    "product_train[\"search_term\"] = product_train[\"search_term\"].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ggplot import *\n",
    "p = ggplot(aes(x='relevance'), data=product_train)\n",
    "p + geom_histogram()\n",
    "\n",
    "\n",
    "from ggplot import *\n",
    "#features[\"relevance\"]\n",
    "p = ggplot(aes(x='relevance'), data=product_train)\n",
    "p + geom_histogram(colour=\"#FF9999\",fill = \"black\")+xlab(\"Relevance (Target)\") +ylab(\"Count\") + ggtitle(\"Population Distribution\")\n",
    "\n",
    "p + theme_bw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = product_train[\"search_term\"] # we only take the first two features in order to easily visualize the results. \n",
    "                      # We could avoid this ugly slicing by using a two-dim dataset\n",
    "y = product_train[\"relevance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_waste, X_sample, y_waste, y_sample = train_test_split(\n",
    "     X, y, test_size=0.01, random_state=0,stratify=y)\n",
    "\n",
    "X_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique = pd.DataFrame(X_sample.unique(), columns=['search_term'])\n",
    "product_train=pd.merge(product_train,unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ggplot import *\n",
    "p = ggplot(aes(x='relevance'), data=product_train)\n",
    "p + geom_histogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Functions\n",
    "def fuzzymatch(x,y,edits):\n",
    "    a = x.split()\n",
    "    b = y.split()\n",
    "    match = 0\n",
    "    \n",
    "    for i in range(0,len(a)):\n",
    "        for j in range(0,len(b)):\n",
    "            if damerau_levenshtein_distance(a[i],  b[j]) <= edits:\n",
    "                match = match + 1\n",
    "                j = j + 1\n",
    "    return(match)\n",
    "\n",
    "def uniquefuzzymatch(x,y,edits):\n",
    "    a = x.split()\n",
    "    b = y.split()\n",
    "    match = 0\n",
    "    \n",
    "    for i in range(0,len(a)):\n",
    "        counter=0\n",
    "        for j in range(0,len(b)):\n",
    "            if damerau_levenshtein_distance(a[i],  b[j]) <= edits:\n",
    "                if counter==0:\n",
    "                    match = match + 1\n",
    "                    counter=counter+1\n",
    "                j = j + 1\n",
    "    return(match)\n",
    "\n",
    "\n",
    "\n",
    "def position_match(x,y,pos,edits):\n",
    "    a = x.split()\n",
    "    b = y.split()\n",
    "    if pos==\"first\":\n",
    "        word=a[0]\n",
    "    else:\n",
    "        word=a[len(a)-1]\n",
    "    position = 0\n",
    "    counter=0\n",
    "    for j in range(0,len(b)):\n",
    "        if damerau_levenshtein_distance(word,  b[j]) <= edits:\n",
    "            if counter==0:\n",
    "                position=j\n",
    "                counter=1\n",
    "            \n",
    "    return(position)\n",
    "\n",
    "def match_query(x,y):\n",
    "    if x in y:\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "\n",
    "def jacc_dist(x,y,edits):\n",
    "    num=uniquefuzzymatch(x,y,edits)\n",
    "    den=len(x.split())+len(y.split())-num\n",
    "    return(float(num)/float(den))\n",
    "\n",
    "def dice_dist(x,y,edits):\n",
    "    num=uniquefuzzymatch(x,y,edits)\n",
    "    den=len(x.split())+len(y.split())\n",
    "    return(float(2*num)/float(den))\n",
    "\n",
    "def cosine_dist(x,y,edits):\n",
    "    num=uniquefuzzymatch(x,y,edits)\n",
    "    den=math.sqrt(len(x.split()))*math.sqrt(len(y.split()))\n",
    "    return(float(2*num)/float(den))\n",
    "\n",
    "\n",
    "def makeFeatureVec(frame, model, num_features,algo):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    words=frame.split()\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed\n",
    "    if algo==\"word2vec\":\n",
    "        index2word_set = set(model.index2word)\n",
    "    else:\n",
    "        index2word_set=vectorizer.get_feature_names()\n",
    "        lsa_matrix=pd.DataFrame(lsa.components_,columns =vectorizer.get_feature_names())\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            if algo==\"word2vec\":\n",
    "                featureVec = np.add(featureVec,model[word])\n",
    "            else:\n",
    "                featureVec = np.add(featureVec,lsa_matrix[word].values)\n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "\n",
    "def tfidfFeatureVec(frame, model, num_features,tfidf_dict,algo):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    words=frame.split()\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    if algo==\"word2vec\":\n",
    "        index2word_set = set(model.index2word)\n",
    "    else:\n",
    "        index2word_set=vectorizer.get_feature_names()\n",
    "        lsa_matrix=pd.DataFrame(lsa.components_,columns =vectorizer.get_feature_names())\n",
    "\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set and word in tfidf_dict.keys(): \n",
    "            nwords = nwords + 1.\n",
    "            if algo==\"word2vec\":\n",
    "                featureVec = np.add(featureVec,model[word]*tfidf_dict[word])\n",
    "            else:\n",
    "                featureVec = np.add(featureVec,lsa_matrix[word].values*tfidf_dict[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "    \n",
    "\n",
    "def ngram_count(text1, text2,n):\n",
    "    count= 0\n",
    "    trigrams1 = ngrams(text1.split(), n)\n",
    "    for grams1 in trigrams1:\n",
    "        trigrams2 = ngrams(text2.split(), n)\n",
    "        for grams2 in trigrams2:\n",
    "            if n==2:\n",
    "                if damerau_levenshtein_distance(grams1[0],grams2[0])<=2 and damerau_levenshtein_distance(grams1[1],grams2[1])<=2:\n",
    "                    count=count+1\n",
    "            else:\n",
    "                if damerau_levenshtein_distance(grams1[0],grams2[0])<=2 and damerau_levenshtein_distance(grams1[1],grams2[1])<=2\\\n",
    "                and damerau_levenshtein_distance(grams1[2],grams2[2])<=2:\n",
    "                    count=count+1\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prod_desc = pd.merge(product_train, desc_clean, on=\"product_uid\")\n",
    "#prod_desc=prod_desc.head()\n",
    "\n",
    "#Calculate tfidf \n",
    "vectorizer = TfidfVectorizer(min_df = 1)\n",
    "vectorizer.fit_transform(desc_clean[\"product_description\"])\n",
    "tfidf_dict={t[0]:t[1] for t in vectorizer.vocabulary_.items()}\n",
    "\n",
    "\n",
    "#Make description corpus\n",
    "corpus=[]\n",
    "for i in range(shape(desc_clean)[0]):\n",
    "    corpus.append(desc_clean[\"product_description\"][i].split(\" \"))\n",
    "model = gensim.models.Word2Vec(corpus, size=100,window=5, min_count=5,seed=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 1,max_features=1000)\n",
    "dtm = vectorizer.fit_transform(desc_clean[\"product_description\"])\n",
    "lsa = TruncatedSVD(100, algorithm = 'randomized',random_state=0)\n",
    "dtm_lsa = lsa.fit_transform(dtm)\n",
    "dtm_lsa = Normalizer(copy=False).fit_transform(dtm_lsa)\n",
    "name=range(1,101)\n",
    "names=['lsa_'+str(col)   for col in name]\n",
    "lsa_matrix=pd.DataFrame(lsa.components_,index=names,columns =vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "prod_desc[\"fuzzymatch_desc\"]=prod_desc.apply(lambda x: fuzzymatch(x['search_term'],x['product_description'],2),axis=1)\n",
    "prod_desc[\"match_desc\"]=prod_desc.apply(lambda x: fuzzymatch(x['search_term'],x['product_description'],0),axis=1)\n",
    "prod_desc[\"uniquefuzzymatch_desc\"]=prod_desc.apply(lambda x: uniquefuzzymatch(x['search_term'],x['product_description'],2),axis=1)\n",
    "prod_desc[\"uniquematch_desc\"]=prod_desc.apply(lambda x: uniquefuzzymatch(x['search_term'],x['product_description'],0),axis=1)\n",
    "\n",
    "prod_desc[\"fuzzymatch_title\"]=prod_desc.apply(lambda x: fuzzymatch(x['search_term'],x['product_title'],2),axis=1)\n",
    "prod_desc[\"match_title\"]=prod_desc.apply(lambda x: fuzzymatch(x['search_term'],x['product_title'],0),axis=1)\n",
    "prod_desc[\"uniquefuzzymatch_title\"]=prod_desc.apply(lambda x: uniquefuzzymatch(x['search_term'],x['product_title'],2),axis=1)\n",
    "prod_desc[\"uniquematch_title\"]=prod_desc.apply(lambda x: uniquefuzzymatch(x['search_term'],x['product_title'],0),axis=1)\n",
    "\n",
    "prod_desc[\"fuzzymatch_attr\"]=prod_desc.apply(lambda x: fuzzymatch(x['search_term'],x['product_attribute_concat'],2),axis=1)\n",
    "prod_desc[\"match_attr\"]=prod_desc.apply(lambda x: fuzzymatch(x['search_term'],x['product_attribute_concat'],0),axis=1)\n",
    "prod_desc[\"uniquefuzzymatch_attr\"]=prod_desc.apply(lambda x: uniquefuzzymatch(x['search_term'],x['product_attribute_concat'],2),axis=1)\n",
    "prod_desc[\"uniquematch_attr\"]=prod_desc.apply(lambda x: uniquefuzzymatch(x['search_term'],x['product_attribute_concat'],0),axis=1)\n",
    "\n",
    "#First position as most of the queries are color (attributes)\n",
    "prod_desc[\"first_pos_desc_exact\"]=prod_desc.apply(lambda x: position_match(x['search_term'],x['product_description'],\"first\",0),axis=1)\n",
    "prod_desc[\"first_pos_title_exact\"]=prod_desc.apply(lambda x: position_match(x['search_term'],x['product_title'],\"first\",0),axis=1)\n",
    "prod_desc[\"first_pos_brand_exact\"]=prod_desc.apply(lambda x: position_match(x['search_term'],x['product_brand'],\"first\",0),axis=1)\n",
    "prod_desc[\"first_pos_color_exact\"]=prod_desc.apply(lambda x: position_match(x['search_term'],x['product_color'],\"first\",0),axis=1)\n",
    "prod_desc[\"first_pos_attr_exact\"]=prod_desc.apply(lambda x: position_match(x['search_term'],x['product_attribute_concat'],\\\n",
    "                                                                 \"first\",0),axis=1)\n",
    "\n",
    "prod_desc[\"first_pos_desc_fuzzy\"]=prod_desc.apply(lambda x: position_match(x['search_term'],x['product_description'],\"first\",2),axis=1)\n",
    "prod_desc[\"first_pos_title_fuzzy\"]=prod_desc.apply(lambda x: position_match(x['search_term'],x['product_title'],\"first\",2),axis=1)\n",
    "prod_desc[\"first_pos_brand_fuzzy\"]=prod_desc.apply(lambda x: position_match(x['search_term'],x['product_brand'],\"first\",2),axis=1)\n",
    "prod_desc[\"first_pos_color_fuzzy\"]=prod_desc.apply(lambda x: position_match(x['search_term'],x['product_color'],\"first\",2),axis=1)\n",
    "prod_desc[\"first_pos_attr_fuzzy\"]=prod_desc.apply(lambda x: position_match(x['search_term'],x['product_attribute_concat'],\\\n",
    "                                                                 \"first\",2),axis=1)\n",
    "\n",
    "#Last position with desc and title as most of the queries have last word as product\n",
    "prod_desc[\"last_pos_desc_exact\"]=prod_desc.apply(lambda x: position_match(x['search_term'],x['product_description'],\"last\",0),axis=1)\n",
    "prod_desc[\"last_pos_title_exact\"]=prod_desc.apply(lambda x: position_match(x['search_term'],x['product_title'],\"last\",0),axis=1)\n",
    "prod_desc[\"last_pos_brand_exact\"]=prod_desc.apply(lambda x: position_match(x['search_term'],x['product_brand'],\"last\",0),axis=1)\n",
    "prod_desc[\"last_pos_color_exact\"]=prod_desc.apply(lambda x: position_match(x['search_term'],x['product_color'],\"last\",0),axis=1)\n",
    "prod_desc[\"last_pos_attr_exact\"]=prod_desc.apply(lambda x: position_match(x['search_term'],x['product_attribute_concat'],\"last\",0),axis=1)\n",
    "\n",
    "prod_desc[\"last_pos_desc_fuzzy\"]=prod_desc.apply(lambda x: position_match(x['search_term'],x['product_description'],\"last\",2),axis=1)\n",
    "prod_desc[\"last_pos_title_fuzzy\"]=prod_desc.apply(lambda x: position_match(x['search_term'],x['product_title'],\"last\",2),axis=1)\n",
    "prod_desc[\"last_pos_brand_fuzzy\"]=prod_desc.apply(lambda x: position_match(x['search_term'],x['product_brand'],\"last\",2),axis=1)\n",
    "prod_desc[\"last_pos_color_fuzzy\"]=prod_desc.apply(lambda x: position_match(x['search_term'],x['product_color'],\"last\",2),axis=1)\n",
    "prod_desc[\"last_pos_attr_fuzzy\"]=prod_desc.apply(lambda x: position_match(x['search_term'],x['product_attribute_concat'],\"last\",2),axis=1)\n",
    "\n",
    "#Match the whole search query\n",
    "prod_desc[\"query_match_desc\"]=prod_desc.apply(lambda x: match_query(x['search_term'],x['product_description']),axis=1)\n",
    "prod_desc[\"query_match_title\"]=prod_desc.apply(lambda x: match_query(x['search_term'],x['product_title']),axis=1)\n",
    "prod_desc[\"query_match_attr\"]=prod_desc.apply(lambda x: match_query(x['search_term'],x['product_attribute_concat']),axis=1)\n",
    "\n",
    "#Distances\n",
    "prod_desc[\"jacc_title_exact\"]=prod_desc.apply(lambda x: jacc_dist(x['search_term'],x['product_title'],0),axis=1)\n",
    "prod_desc[\"dice_title_exact\"]=prod_desc.apply(lambda x: dice_dist(x['search_term'],x['product_title'],0),axis=1)\n",
    "prod_desc[\"cosine_title_exact\"]=prod_desc.apply(lambda x: cosine_dist(x['search_term'],x['product_title'],0),axis=1)\n",
    "prod_desc[\"jacc_title_fuzzy\"]=prod_desc.apply(lambda x: jacc_dist(x['search_term'],x['product_title'],2),axis=1)\n",
    "prod_desc[\"dice_title_fuzzy\"]=prod_desc.apply(lambda x: dice_dist(x['search_term'],x['product_title'],2),axis=1)\n",
    "prod_desc[\"cosine_title_fuzzy\"]=prod_desc.apply(lambda x: cosine_dist(x['search_term'],x['product_title'],2),axis=1)\n",
    "\n",
    "prod_desc[\"jacc_desc_exact\"]=prod_desc.apply(lambda x: jacc_dist(x['search_term'],x['product_description'],0),axis=1)\n",
    "prod_desc[\"dice_desc_exact\"]=prod_desc.apply(lambda x: dice_dist(x['search_term'],x['product_description'],0),axis=1)\n",
    "prod_desc[\"cosine_desc_exact\"]=prod_desc.apply(lambda x: cosine_dist(x['search_term'],x['product_description'],0),axis=1)\n",
    "prod_desc[\"jacc_desc_fuzzy\"]=prod_desc.apply(lambda x: jacc_dist(x['search_term'],x['product_description'],2),axis=1)\n",
    "prod_desc[\"dice_desc_fuzzy\"]=prod_desc.apply(lambda x: dice_dist(x['search_term'],x['product_description'],2),axis=1)\n",
    "prod_desc[\"cosine_desc_fuzzy\"]=prod_desc.apply(lambda x: cosine_dist(x['search_term'],x['product_description'],2),axis=1)\n",
    "\n",
    "\n",
    "prod_desc[\"jacc_attr_exact\"]=prod_desc.apply(lambda x: jacc_dist(x['search_term'],x['product_attribute_concat'],0),axis=1)\n",
    "prod_desc[\"dice_attr_exact\"]=prod_desc.apply(lambda x: dice_dist(x['search_term'],x['product_attribute_concat'],0),axis=1)\n",
    "prod_desc[\"cosine_attr_exact\"]=prod_desc.apply(lambda x: cosine_dist(x['search_term'],x['product_attribute_concat'],0),axis=1)\n",
    "prod_desc[\"jacc_attr_fuzzy\"]=prod_desc.apply(lambda x: jacc_dist(x['search_term'],x['product_attribute_concat'],2),axis=1)\n",
    "prod_desc[\"dice_attr_fuzzy\"]=prod_desc.apply(lambda x: dice_dist(x['search_term'],x['product_attribute_concat'],2),axis=1)\n",
    "prod_desc[\"cosine_attr_fuzzy\"]=prod_desc.apply(lambda x: cosine_dist(x['search_term'],x['product_attribute_concat'],2),axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#word2vec vectors\n",
    "prod_desc['title_word_vector'] = prod_desc['product_title'].apply(lambda x: makeFeatureVec(x,model,100,\"word2vec\"))\n",
    "prod_desc['desc_word_vector'] = prod_desc['product_description'].apply(lambda x: makeFeatureVec(x,model,100,\"word2vec\"))\n",
    "prod_desc['search_word_vector'] = prod_desc['search_term'].apply(lambda x: makeFeatureVec(x,model,100,\"word2vec\"))\n",
    "prod_desc['attr_word_vector'] = prod_desc['product_attribute_concat'].apply(lambda x: makeFeatureVec(x,model,100,\"word2vec\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(prod_desc['search_term'])):\n",
    "    if np.isnan(prod_desc[\"search_word_vector\"][i][1]):\n",
    "        prod_desc.drop(i,inplace=True)\n",
    "prod_desc = prod_desc.reset_index(drop=True)\n",
    "\n",
    "\n",
    "for i in range(0,len(prod_desc['search_term'])):\n",
    "    if np.isnan(prod_desc[\"attr_word_vector\"][i][1]):\n",
    "        prod_desc.drop(i,inplace=True)\n",
    "prod_desc = prod_desc.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "prod_desc['cos_title_word'] = prod_desc.apply(lambda x: spatial.distance.cosine(x[\"search_word_vector\"],x[\"title_word_vector\"]),\\\n",
    "                                              axis=1)\n",
    "prod_desc['cos_desc_word'] = prod_desc.apply(lambda x: spatial.distance.cosine(x[\"search_word_vector\"],x[\"desc_word_vector\"]),\\\n",
    "                                             axis=1)\n",
    "prod_desc['cos_attr_word'] = prod_desc.apply(lambda x: spatial.distance.cosine(x[\"search_word_vector\"],x[\"attr_word_vector\"]),\\\n",
    "                                             axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(prod_desc['search_term'])):\n",
    "    if np.isnan(prod_desc[\"tfidf_search_word_vector\"][i][1]):\n",
    "        prod_desc.drop(i,inplace=True)\n",
    "prod_desc = prod_desc.reset_index(drop=True)\n",
    "\n",
    "\n",
    "for i in range(0,len(prod_desc['search_term'])):\n",
    "    if np.isnan(prod_desc[\"tfidf_attr_word_vector\"][i][1]):\n",
    "        prod_desc.drop(i,inplace=True)\n",
    "prod_desc = prod_desc.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prod_desc['tfidf_title_word_vector'] = prod_desc['product_title'].apply\\\n",
    "(lambda x: tfidfFeatureVec(x,model,100,tfidf_dict,\"word2vec\"))\n",
    "prod_desc['tfidf_desc_word_vector'] = prod_desc['product_description'].apply\\\n",
    "(lambda x: tfidfFeatureVec(x,model,100,tfidf_dict,\"word2vec\"))\n",
    "prod_desc['tfidf_search_word_vector'] = prod_desc['search_term'].apply\\\n",
    "(lambda x: tfidfFeatureVec(x,model,100,tfidf_dict,\"word2vec\"))\n",
    "prod_desc['tfidf_attr_word_vector'] = prod_desc['product_attribute_concat'].apply\\\n",
    "(lambda x: tfidfFeatureVec(x,model,100,tfidf_dict,\"word2vec\"))\n",
    "\n",
    "\n",
    "prod_desc['tfidf_cos_title_word'] = prod_desc.apply(lambda x: spatial.distance.cosine(x[\"tfidf_search_word_vector\"],\\\n",
    "                                                                        x[\"tfidf_title_word_vector\"]),axis=1)\n",
    "prod_desc['tfidf_cos_desc_word'] = prod_desc.apply(lambda x: spatial.distance.cosine(x[\"tfidf_search_word_vector\"],\\\n",
    "                                                                           x[\"tfidf_desc_word_vector\"]),axis=1)\n",
    "prod_desc['tfidf_cos_attr_word'] = prod_desc.apply(lambda x: spatial.distance.cosine(x[\"tfidf_search_word_vector\"],\\\n",
    "                                                                           x[\"tfidf_attr_word_vector\"]),axis=1)\n",
    "\n",
    "#lsa vectors\n",
    "prod_desc['title_lsa_vector'] = prod_desc['product_title'].apply(lambda x: makeFeatureVec(x,vectorizer,100,\"lsa\"))\n",
    "prod_desc['desc_lsa_vector'] = prod_desc['product_description'].apply(lambda x: makeFeatureVec(x,vectorizer,100,\"lsa\"))\n",
    "prod_desc['search_lsa_vector'] = prod_desc['search_term'].apply(lambda x: makeFeatureVec(x,vectorizer,100,\"lsa\"))\n",
    "prod_desc['attr_lsa_vector'] = prod_desc['product_attribute_concat'].apply(lambda x: makeFeatureVec(x,vectorizer,100,\"lsa\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prod_desc['cos_title_lsa'] = prod_desc.apply(lambda x: spatial.distance.cosine(x[\"search_lsa_vector\"],x[\"title_lsa_vector\"]),\\\n",
    "                                             axis=1)\n",
    "prod_desc['cos_desc_lsa'] = prod_desc.apply(lambda x: spatial.distance.cosine(x[\"search_lsa_vector\"],x[\"desc_lsa_vector\"]),\\\n",
    "                                            axis=1)\n",
    "prod_desc['cos_attr_lsa'] = prod_desc.apply(lambda x: spatial.distance.cosine(x[\"search_lsa_vector\"],x[\"attr_lsa_vector\"]),\\\n",
    "                                            axis=1)\n",
    "\n",
    "prod_desc['tfidf_title_lsa_vector'] = prod_desc['product_title'].apply\\\n",
    "(lambda x: tfidfFeatureVec(x,vectorizer,100,tfidf_dict,\"lsa\"))\n",
    "prod_desc['tfidf_desc_lsa_vector'] = prod_desc['product_description'].apply\\\n",
    "(lambda x: tfidfFeatureVec(x,vectorizer,100,tfidf_dict,\"lsa\"))\n",
    "prod_desc['tfidf_search_lsa_vector'] = prod_desc['search_term'].apply\\\n",
    "(lambda x: tfidfFeatureVec(x,vectorizer,100,tfidf_dict,\"lsa\"))\n",
    "prod_desc['tfidf_attr_lsa_vector'] = prod_desc['product_attribute_concat'].apply\\\n",
    "(lambda x: tfidfFeatureVec(x,vectorizer,100,tfidf_dict,\"lsa\"))\n",
    "\n",
    "prod_desc['tfidf_cos_title_lsa'] = prod_desc.apply(lambda x: spatial.distance.cosine(x[\"tfidf_search_lsa_vector\"],\\\n",
    "                                                                        x[\"tfidf_title_lsa_vector\"]),axis=1)\n",
    "prod_desc['tfidf_cos_desc_lsa'] = prod_desc.apply(lambda x: spatial.distance.cosine(x[\"tfidf_search_lsa_vector\"],\\\n",
    "                                                                           x[\"desc_lsa_vector\"]),axis=1)\n",
    "prod_desc['tfidf_cos_attr_lsa'] = prod_desc.apply(lambda x: spatial.distance.cosine(x[\"tfidf_search_lsa_vector\"],\\\n",
    "                                                                           x[\"tfidf_attr_lsa_vector\"]),axis=1)\n",
    "\n",
    "#n-grams\n",
    "prod_desc[\"bigram_title\"]=prod_desc.apply(lambda x: ngram_count(x[\"search_term\"],x[\"product_title\"],2),axis=1)\n",
    "prod_desc[\"bigram_desc\"]=prod_desc.apply(lambda x: ngram_count(x[\"search_term\"],x[\"product_description\"],2),axis=1)\n",
    "prod_desc[\"bigram_attr\"]=prod_desc.apply(lambda x: ngram_count(x[\"search_term\"],x[\"product_attribute_concat\"],2),axis=1)\n",
    "\n",
    "prod_desc[\"trigram_title\"]=prod_desc.apply(lambda x: ngram_count(x[\"search_term\"],x[\"product_title\"],3),axis=1)\n",
    "prod_desc[\"trigram_desc\"]=prod_desc.apply(lambda x: ngram_count(x[\"search_term\"],x[\"product_description\"],3),axis=1)\n",
    "prod_desc[\"trigram_attr\"]=prod_desc.apply(lambda x: ngram_count(x[\"search_term\"],x[\"product_attribute_concat\"],3),axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prod_desc['char_count'] = prod_desc['search_term'].apply(lambda x: sum(char.isalpha() for char in str(x)))\n",
    "prod_desc['digits_perc'] = (prod_desc['search_term'].apply(lambda x: sum(char.isdigit() for char in str(x)))*100)/ \\\n",
    "                        prod_desc['search_term'].apply(lambda x: len(str(x).replace(\" \",\"\")))\n",
    "prod_desc['color_match'] = prod_desc.apply(lambda x: match_query(x['product_color'],x['search_term']),axis=1)\n",
    "prod_desc['brand_match'] = prod_desc.apply(lambda x: match_query(x['product_brand'],x['search_term'],),axis=1)\n",
    "prod_desc['products_per_query'] = prod_desc.groupby(\"search_term\")['product_uid'].count().reset_index(drop=True)\n",
    "prod_desc['query_per_product'] = prod_desc.groupby(\"product_uid\")['search_term'].count().reset_index(drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prod_desc.to_csv('features.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
